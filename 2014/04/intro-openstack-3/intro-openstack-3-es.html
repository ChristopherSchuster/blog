Introducci&oacute;n a OpenStack para Ubuntu Parte III

<p>Es es el tercer Post de introducci&oacute;n a Open Stack. <a href="http://www.maigfrga.ntweb.co/openstack-introduction-ubuntu-part-i/"> La primer parte de esta serie es sobre conceptos generales, confiuraci&oacute;n b&aacute;sica e instalaci&oacute;n del Identity service </a>, 
<a href="http://www.maigfrga.ntweb.co/openstack-introduction-ubuntu-part-ii/"> la segunda parte continua con Image y compute services.</a> Esta parte es sobre el dashboard y block storage.
<a href="http://docs.openstack.org/havana/install-guide/install/apt/content/">En este enlace se puede encontrar la gu&iacute;a completa.</a>
</p>


<h3>Instalando el Dashboard </h3>


<p>
El dashboard is a una app web que permite a usuarios y administradores administrar los servicios y recursos
de OpenStack. Esta app permite interacci&oacute;n con el OpenStack Compute cloud controller a trav&ecute;s de las
APIs.
</p>

<p>
   Instalar apache server y el dashboard en el controller:
</p>

<pre>
   # apt-get install apache2 libapache2-mod-wsgi  memcached  openstack-dashboard 
</pre>

<pre>
   # apt-get remove --purge openstack-dashboard-ubuntu-theme 
</pre>

<p>
Modifar el valor de CACHES['default']['LOCATION'] en /etc/openstack-dashboard/local_settings.py para coincider
con los valores definidos en  /etc/memcached.conf
</p>

<pre>
   CACHES = {
    'default': {
        'BACKEND' : 'django.core.cache.backends.memcached.MemcachedCache',
        'LOCATION' : '127.0.0.1:11211'
        }
    }
</pre>

<p>
   Actualizar el parametro ALLOWED_HOSTS en local_settings.py  para incluid las direcciones desde las cuales se
   desea acceder al dashboard.
</p>

<pre>
   ALLOWED_HOSTS = ['localhost', 'my-desktop'] 
   OPENSTACK_HOST = "controller" 
</pre>

<p>
El dashboard es accesible en  http://controller/horizon .
Las credenciales de acceso son las usadas por alguno de los usuarios creado con el OpenStack Identity Service.
</p>

<h3>Agregar el Block Storage Service</h3>


<p>
   OpenStack Block Storage Service funciona a trav&eacute;s de la interacci&oacute;n de una serie de procesos
   llamados cinder-* que residen en la(s) mquina(s) . Es posible desplegar estos servicios en uno o varios nodos . 
 </p>

<h4>Block Storage Service</h4>
<p>
 Block Storage Service permite el manejo de volumenes, snapshots, y tipos de volumenes. Incluye los siguientes
 componentes:
</p>

<ul>
    <li>cinder-api: Acepta API requests y las mapea a los procesos cinder-volume. 
    </li>
    <li>
       cinder-volume: Responde requests de lectura y escritura a la base de datos  Block Storage . 
    </li>
    <li>
    cinder-scheduler daemon: Selecciona el proveedor de block storage optimo para crear el volumen.
    </li>
    <li>
       Messaging queue: Rutea informaci&oacute;n entre los procesos de Block Storage Service. 
    </li>
</ul>



<h4>Configurar Block Storage Service controller</h4>
<p> Instalar paquetes </p>
<pre>
   # apt-get install cinder-api cinder-scheduler 
</pre>
<p>
Configurar Block Storage para usar  MySQL database. Editar  /etc/cinder/cinder.conf y agregar la siguiente
configuraci&oacute;n en la secci&oacute;n  [database] 
</p>

<pre>
   [database]
    ...
    connection = mysql://cinder:CINDER_DBPASS@controller/cinder 
</pre>

<p>Crear base de datos y usario</p>
<p>
   # mysql -u root -p
    mysql> CREATE DATABASE cinder;
    mysql> GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'localhost' \
    IDENTIFIED BY 'CINDER_DBPASS';
    mysql> GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'%' \
    IDENTIFIED BY 'CINDER_DBPASS'; 
</p>

<p>Crear tablas </p>

<pre>
   # cinder-manage db sync 
</pre>

<p>Create a usuario cinder en keystone</p>
<pre>
   # keystone user-create --name=cinder --pass=CINDER_PASS --email=cinder@example.com
    +----------+----------------------------------+
    | Property |              Value               |
    +----------+----------------------------------+
    |  email   |        cinder@example.com        |
    | enabled  |               True               |
    |    id    | ff2562ccad9d490aaf214e5e5e063186 |
    |   name   |              cinder              |
    +----------+----------------------------------+
   
   # keystone user-role-add --user=cinder --tenant=service --role=admin 
</pre>


<p>
Agregar credenciales al archivo /etc/cinder/api-paste.ini en la secci&oacute;n [filter:authtoken]
</p>

<pre>
    [filter:authtoken]
    paste.filter_factory=keystoneclient.middleware.auth_token:filter_factory
    auth_host=controller
    auth_port = 35357
    auth_protocol = http
    auth_uri = http://controller:5000
    admin_tenant_name=service
    admin_user=cinder
    admin_password=CINDER_PASS
</pre>


<p>
Configurar Block Storage para usar RabbitMQ message broker. Edital la secci&oacute;n [DEFAULT] en el archivo /etc/cinder/cinder.conf 
</p>

<pre>
   [DEFAULT]
    ...
    rpc_backend = cinder.openstack.common.rpc.impl_kombu
    rabbit_host = controller
    rabbit_port = 5672
    rabbit_userid = guest
    rabbit_password = RABBIT_PASS 
</pre>


<p>
   Registrar el servicio Block Storage Service en el Identity Service para que los servicios de OpenStack puedan
   localizarlo. 
</p>

<pre>
   # keystone service-create --name=cinder --type=volume --description="Cinder Volume Service" 
    +-------------+----------------------------------+
    |   Property  |              Value               |
    +-------------+----------------------------------+
    | description |      Cinder Volume Service       |
    |      id     | 5f4ac31dd0334c9d89e72375f165c26d |
    |     name    |              cinder              |
    |     type    |              volume              |
    +-------------+----------------------------------+
</pre>

<p>Usar la propiedad id para crear el endpoint  </p>

<pre>
   # keystone endpoint-create \
  --service-id=the_service_id_above \
  --publicurl=http://controller:8776/v1/%\(tenant_id\)s \
  --internalurl=http://controller:8776/v1/%\(tenant_id\)s \
  --adminurl=http://controller:8776/v1/%\(tenant_id\)s 
</pre>

<p>
   Registar el servicio y endpoint para la version 2 de Block Storage Service API.
</p>

<pre>
   # keystone service-create --name=cinderv2 --type=volumev2 \
  --description="Cinder Volume Service V2" 

    +-------------+----------------------------------+
    |   Property  |              Value               |
    +-------------+----------------------------------+
    | description |     Cinder Volume Service V2     |
    |      id     | 46d56ead76f94608b5ff8432b156b865 |
    |     name    |             cinderv2             |
    |     type    |             volumev2             |
    +-------------+----------------------------------+

</pre>

<p>Registar endpoint</p>

<pre>
       # keystone endpoint-create \
  --service-id=the_service_id_above \
  --publicurl=http://controller:8776/v1/%\(tenant_id\)s \
  --internalurl=http://controller:8776/v1/%\(tenant_id\)s \
  --adminurl=http://controller:8776/v1/%\(tenant_id\)s 

</pre>


<p>Reiniciar cinder service</p>
<pre>
   # service cinder-scheduler restart
   # service cinder-api restart 
</pre>


<h4> Configurar un nodo Block Storage Service</h4>


<p>
Configuraremos un nuevo nodo que contendr&aacute; el disco que servira los volumenes, para esto es necesario
actualizar el archivo /etc/hosts en el nodo controlador y en nuevo nodo:
</p>

<pre>
    127.0.0.1       localhost
    192.168.0.10    controller
    192.168.0.11    compute1
    192.168.0.12    block1
</pre>

<p>Instalar los paquetes LVM en el nodo the block1</p>

<pre>
   # apt-get install lvm2 
</pre>

<p>
Crear los volumenes fisicos y logicos. Esta gu&iacute;a asume que un segundo disco /dev/sdb es usado para este
proposito.
</p>

<pre>
    # pvcreate /dev/sdb
    # vgcreate cinder-volumes /dev/sdb 
</pre>

<p>
   Agregar una entrada en el filtro LVM en la secci&oacute;n devices en /etc/lvm/lvm.conf  para evitar que 
   LVM utlice dispositivos usados por las maquinas virtuales.
   Cata item en el filtro comienza con una a para aceptar o una r para denegar, el array debe terminar en "r/.*/"
   para denegar dispositivos no listados.

    En este ejemplo /dev/sda1 es el volumen del sistema operativo y /dev/sdb el reservado para volumenes cinder.

</p>
<pre>
   devices {
    ...
    filter = [ "a/sda1/", "a/sdb/", "r/.*/"]
    ...
    } 
</pre>

<p>
    Instalar los paquetes para Block Storage Service en el nodo block1 
</p>

<pre>
   # apt-get install cinder-volume 
</pre>


<p>
Editar /etc/cinder/api-paste.ini , en la secci&oacute;n [filter:authtoken]:
</p>

<pre>
   [filter:authtoken]
    paste.filter_factory=keystoneclient.middleware.auth_token:filter_factory
    auth_host=controller
    auth_port = 35357
    auth_protocol = http
    admin_tenant_name=service
    admin_user=cinder
    admin_password=CINDER_PASS 
</pre>

<p>
Configurar Block Storage para user RabbitMQ message broker editando la secci&oacute;n  [DEFAULT] 
en  /etc/cinder/cinder.conf 
</p>

<pre>
   [DEFAULT]
    ...
    rpc_backend = cinder.openstack.common.rpc.impl_kombu
    rabbit_host = controller
    rabbit_port = 5672
    rabbit_userid = guest
    rabbit_password = RABBIT_PASS 
</pre>

<p>
Configurar Block Storage para usar la bd MySQL. Editar  /etc/cinder/cinder.conf 
</p>

<pre>
   [database]
    ...
    connection = mysql://cinder:CINDER_DBPASS@controller/cinder 
</pre>

<p> Reiniciar cinder service:   </p>

<pre>
   # service cinder-volume restart
   # service tgt restart 
</pre>



