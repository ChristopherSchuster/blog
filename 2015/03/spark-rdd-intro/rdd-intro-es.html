<p>
    Spark Resilient Distributed Dataset (RDD) son colecciones de datos distribuidas y 
    resistentens a fallos que pueden ser operadas en paralelo. Este post es una introducci&oacute;n
    a las operaciones m&aacute;s b&aacute;sicas que podemos hacer con estas colecciones.

</p>

<h2> Primeros pasos con RDD </h2>
<p>
    Una colecci&oacute;n RDD es inmutable y generalmente se encuentra distribuida en m&uacute;ltiples 
    particiones en un cluster, para propositos de prueba podemos crear una colecci&oacute;n RDD en memoria:
</p>

<pre>
    sc = SparkContext("local", "Rdd intro")
    data = [x for x in xrange(100)]
    distData = sc.parallelize(data)
</pre>


<p>
    Podemos maniuplar distData para obtener una colecci&oacute;n de n&uacute;meros primos:
</p>

<pre>

    def is_prime(n):
        if n in (1, 2 ,3):
            return True

        if n == 4:
            data = [2]
        else:
            data = [x for x in xrange(2, n/2)]

        n = n * 1.0
        for d in data:
            if n % d == 0.0:
                return False
        return True

    primeData = distData.filter(is_prime)
</pre>

<p>
    RDD son computadas cada vez que son accedidas, si queremo usar la misma colecci&oacute;n m&aacute;s de 
    una vez, podemos persistirla:
</p>

<pre>

def get_fibonacci(n):
    return ((1+sqrt(5))**n-(1-sqrt(5))**n)/(2**n*sqrt(5))

primeData.persist(StorageLevel.MEMORY_ONLY)
fibonacciData = primeData.map(get_fibonacci)
print fibonacciData.collect()
</pre>

<h2> Usando Spark RDD para analizar base de datos open data colombia </h2>
<p>
<a href="http://www.maigfrga.ntweb.co/descargando-base-de-datos-open-data-colombia-usando-python/"> En un post
previo</a> aprendimos como desgargar la base de datos  we learn   Open Data Colombia, ahora podemos usar
spark para analizar esta informaci&oacute;n.
No sabemos que tipo de datos estan almacenados, solo sabemos que estan en formato json, por lo tanto un 
punto de partida puede ser analizar las claves que aparecen en estos archivos:
</p>

<pre>

    from operator import add
    from pyspark import SparkContext, StorageLevel
    from sets import Set
    import json
    import os



    sc = SparkContext("local", "Open Data Colombia")


    def to_json(line):
        try:
            j = json.loads(line)
            return j
        except Exception as e:
            return {'error': line}

    def extract_all_keys(structure):
        if not structure:
            return []
        key_set = []
        def traverse(k_set, st):
            for key, value in st.iteritems():
                k_set.append(key)
                if dict == type(value):
                    traverse(k_set, value)
                elif list == type(value):
                    for v in value:
                        traverse(k_set, v)
            return k_set
        return traverse(key_set, structure)

    def merge_keys(l1, l2):
        return l1 + l2

    def process_files(data_dir):
        """
        Step 1. Load json files from location directory.
        Step 2. Aply mapping function to extract all keys by file
        Step 3. Count how many times a key exists on all files
        Step 4. Order dataset
        """
        input = sc.textFile(data_dir)
        data = input.map(
            to_json).map(
                extract_all_keys).flatMap(
                    lambda x: [(k, 1) for k in x]).reduceByKey(
                        lambda x, y: x + y)

        print 'there are %s unique keys\n' % data.count()

        d2 = data.map(lambda x: (x[1], x[0],)).sortByKey(False)
        print d2.take(100)
        sc.stop()

    def main(data_dir):
        process_files(data_dir)

    if __name__ == "__main__":
        # directory where opendata catalog is stored
        open_data_dir = '/notempo/opendata/*'
        main(open_data_dir)

</pre>


<h2> Referencias </h2>

<ul>
    <li>
        <a href="https://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds"> Documentaci&oacute;n Apache Spark </a>
    </li>
    <li>
        <a href="http://www.maigfrga.ntweb.co/introduccion-apache-spark/"> Introduccci&oacute;n Apache Spark  </a>
    </li>
    <li>
        <a href="http://www.maigfrga.ntweb.co/descargando-base-de-datos-open-data-colombia-usando-python/"> 
            Descargando base de datos open data Colombia usando python   </a>
    </li>
</ul>
