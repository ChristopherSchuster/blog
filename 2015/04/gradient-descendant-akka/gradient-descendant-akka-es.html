<div style="width:100%; float:left; padding: 0 5px;">

    <img style="width:29%; float:left;" class="img-thumbnail" src="http://upload.wikimedia.org/wikipedia/commons/6/68/Gradient_ascent_%28surface%29.png" />
    <p style="width:69%; float:left; padding:0 3px;">
      <a href="https://www.youtube.com/watch?v=umAeJ7LMCfU">Gradiente descendiente </a> es un algoritmo
      que minimiza funciones, inicia en un punto aleatorio e itera hasta encontrar el m&iacute;nimo 
      margen de error. Este algorimo es usado para calcular margen de error en an&aacute;lisis estad&iacute;sticos
      tales como regresi&oacute;n linear.
      En este post implmentaremos el algorimto de gradiente descendiente usando
       <a href="http://www.maigfrga.ntweb.co/akka-toolkit-introduction/">akka toolkit. </a>

    </p>

</div>

<h2> Regresi&oacute;n linear y Funci&oacute;n de costo  </h2>
<p>
    Podemo usar la ecuaci&oacute;n standard <code> y = mx + b</code>  para modelar nuestro conjunto de 
    datos, sin embargo, la presici&oacute;n de nuestro modelo depende de que tan adecuados son los valores
     <code> m </code> and <code> b </code> , en otras palabras, necesitamos encontrar los mejores valores
    <code> m </code> y <code> b </code>. La <strong> funci&oacute;n de costo J (θ0, θ1)</strong> es usada
    para medir que tan bien se adaptan los valores <code> m </code> y <code> b </code> a nuestro conjunto de datos.
</p>

<p>
    Podemos calcular nuestra funci&oacute;n de costo iterando sobre todos los puntos  (x, y) y sumando
    la diferencia al cuadrado de cada punto (y) con el valor candidato calculado como ( mx + b).
</p>

<pre>
  case class Point(x: Double, y: Double)
  case class Intercept(theta0: Double, theta1: Double, cost: Double)

  //y = mx + b
  //hθ(x)= θ0 + θ1x
  def calculateCost(intercept: Intercept, numberList: List[Point]): Intercept = {
    // Calculte error for a particular set of θ0  and θ1
    var totalCost: Double = 0.0
    for (i <- 0 to numberList.size) {
      totalCost += math.pow( (numberList{i}.y - (intercept.theta1 *  numberList{i}.x + intercept.theta0)), 2)
    }
    val result = Intercept(intercept.theta0, intercept.theta1, totalCost / numberList.size)
    return result
  }
</pre>

<h2> Algoritmo de Gradiente descendiente  </h2>


<p>
   Este algoritmo es usado para minimzar la funcion de costo,  encontrando los mejores valores (θ0, θ1) 
   para nuestra ecuaci&oacute;n  <code> hθ(x)= θ0 + θ1x </code>.
</p>

<p>
    Antes de empezar a programar, escribiremos los pasos necesarios para implementar este algoritmo:
</p>

<ol>
    <li>
        Seleccionar valores aleatorios iniciales θ0 y θ1.
    </li>

    <li>
        Calcular la gradiente en esta ubicaci&oacute;n
    </li>

    <li>
        Mover la ubicaci&oacute;n en la direcci&oacute;n opuesta a la gradiente, solo un poco. Es decir, tomar
        la gradiente y restarle algun valor <code> alpha </code> el cual ser&aacute; un n&uacute;mero 
        peque&ntilde;o que debe ser pasado por configuraci&oacute;n de modo q sea sencillo cambiarlo
        para ajustar gradualmente el algoritmo.
    </li>

    <li>
        Repetir pasos 2 y 3 hasta que iteraciones adicionales no ayuden mucho.
    </li>
</ol>

<pre>

  def stepGradient(
    intercept: Intercept, numberList: List[Point], learningRate: Double): Intercept = {

    var theta0Gradient: Double = 0.0
    var theta1Gradient: Double = 0.0
    var x: Double = 0.0
    var y: Double = 0.0
    val N = numberList.size.toDouble

    for (i <- 0 to numberList.size) {
      x = numberList{i}.x
      y = numberList{i}.y
      //b gradient
      theta0Gradient += -(2/N) * (y - ((intercept.theta1 * x) + intercept.theta0))
      //m gradient
      theta1Gradient += -(2/N) * x * (y - ((intercept.theta1 * x) + intercept.theta0))
    }
    return Intercept(
      intercept.theta0 - (learningRate * theta0Gradient),
      intercept.theta1 - (learningRate * theta1Gradient), 0.0)
  }

  def calculateGradient(
    intercept: Intercept, numberList: List[Point],
    learningRate: Double, nrOfIterations: Int): Intercept = {

    log.info("Calculating gradient with starting values {} {}", intercept.theta0, intercept.theta1)
    var result: Intercept = intercept
    for (i <- 0 to nrOfIterations) {
      result = stepGradient(result, numberList, learningRate)
    }
    log.info("Gradient result {} {}", result.theta0, result.theta1)
    return result;
  }
</pre>

<h2> Implementaci&oacute;n del algoritmo usando Akka toolkit </h2>

<p>
 Vamos a seguir los pasos descritos en mi post previo a cerca 
 <a href="http://www.maigfrga.ntweb.co/akka-toolkit-introduction/"> akka toolkit. </a> Por lo tanto no 
 cubrir&eacute; temas b&aacute;sicos como instalaci&oacute;n.
 
</p>


<p>
    Debemos definir 6 mensajes para implementar el algoritmo:
</p>

<ul>
    <li>
        ComputeIntercept: enviado al Master para iniciar el computo.
    </li>
    <li>
       CalculateGradient: enviado por el actor Master  a los actores Worker actors  para calcular la gradiente.
    </li>
    <li>
       CalculateCost: enviado por el actor Master a los actores Worker para calcular el margen de error.
    </li>
    <li>
        GradientResult: enviado por los actores Worker a el actor  Master conteniendo el resultado de la
        gradiente.
    </li>
    <li>
        CostResult:  enviado por los actores Worker a el actor Master con el resultado del margen de error.
    </li>
    <li>
        InterceptResult: Enviado por el actor Master al actor Listener conteniendo los valores (θ0, θ1)
        para la ecuaci&oacute;n <code> hθ(x)= θ0 + θ1x </code>, el costo de usar estos valores, y cuanto 
        tiempo tomo el calculo.
    </li>
</ul>

<p>
    A continuaci&oacute;n la implementaci&oacute;n completa:
</p>

<pre>

    package com.notempo1320

    import akka.actor._
    import akka.event.Logging
    import akka.routing.RoundRobinRouter

    import org.clapper.argot._
    import ArgotConverters._

    import scala.concurrent.duration._
    import scala.collection.mutable.ArrayBuffer
    import scala.io.{Source, BufferedSource}

    import java.nio.file.{Paths, Files}
    import java.io._

    case class Point(x: Double, y: Double)
    case class Intercept(theta0: Double, theta1: Double, cost: Double)

    object InterceptApp extends App {

      sealed trait InterceptMessage

      case object ComputeIntercept extends InterceptMessage

      case class CalculateCost(intercept: Intercept,
            numberList: List[Point]) extends InterceptMessage

      case class CostResult(intercept: Intercept) extends InterceptMessage

      case class CalculateGradient(
        intercept: Intercept, numberList: List[Point], learningRate: Double,
        nrOfIterations: Int) extends InterceptMessage

      case class GradientResult(intercept: Intercept) extends InterceptMessage

      case class InterceptResult(interceptResult: Intercept, duration: Duration)


      class Worker extends Actor {
          val log = Logging(context.system, this)

          //y = mx + b
          //hθ(x)= θ0 + θ1x
          def calculateCost(intercept: Intercept, numberList: List[Point]): Intercept = {
            // Calculte error for a particular set of θ0  and θ1
            var totalCost: Double = 0.0
            for (i <- 0 to numberList.size -1) {
              totalCost += math.pow( (numberList{i}.y - (intercept.theta1 *  numberList{i}.x + intercept.theta0)), 2)
            }
            val result = Intercept(intercept.theta0, intercept.theta1, totalCost / numberList.size)
            log.info("CalculateCost for intercept {} {} = {}", result.theta0, result.theta1, result.cost)
            return result
          }

          def stepGradient(
            intercept: Intercept, numberList: List[Point], learningRate: Double): Intercept = {

            var theta0Gradient: Double = 0.0
            var theta1Gradient: Double = 0.0
            var x: Double = 0.0
            var y: Double = 0.0
            val N = numberList.size.toDouble

            for (i <- 0 to numberList.size - 1) {
              x = numberList{i}.x
              y = numberList{i}.y
              //b gradient
              theta0Gradient += -(2/N) * (y - ((intercept.theta1 * x) + intercept.theta0))
              //m gradient
              theta1Gradient += -(2/N) * x * (y - ((intercept.theta1 * x) + intercept.theta0))
            }
            return Intercept(
              intercept.theta0 - (learningRate * theta0Gradient),
              intercept.theta1 - (learningRate * theta1Gradient), 0.0)
          }

          def calculateGradient(
            intercept: Intercept, numberList: List[Point],
            learningRate: Double, nrOfIterations: Int): Intercept = {

            log.info("Calculating gradient with starting values {} {}", intercept.theta0, intercept.theta1)
            var result: Intercept = intercept
            for (i <- 0 to nrOfIterations) {
              result = stepGradient(result, numberList, learningRate)
            }
            log.info("Gradient result {} {}", result.theta0, result.theta1)
            return result;
          }

          def receive = {
            // ! means “fire-and-forget”. Also known as tell.
            case CalculateGradient(intercept: Intercept, numberList: List[Point],
              learningRate: Double, nrOfIterations: Int) =>
              sender ! GradientResult(calculateGradient(intercept, numberList, learningRate, nrOfIterations))

            case CalculateCost(intercept: Intercept, numberList: List[Point]) =>
              sender ! CostResult(calculateCost(intercept, numberList))
          }

      }


      class Master(nrOfWorkers: Int, nrOfIterations: Int,
          learningRate: Double, numberList: List[Point], listener: ActorRef)
        extends Actor {
        val log = Logging(context.system, this)
        var initialIntercept: Intercept = _
        var partialResults = ArrayBuffer.empty[Intercept]
        // Every worker will perform nrOfIterations / nrOfWorkers
        var nrOfCalculations: Int = nrOfIterations / nrOfWorkers
        var nrOfResults: Int = 0
        var theta1: Double = 0.0
        val start: Long = System.currentTimeMillis

        override def preStart() = {
          log.info("Starting master")
          super.preStart()
        }

        // create a round-robin router to make it easier to spread out the work between the workers
        val workerRouter = context.actorOf(
          Props[Worker].withRouter(RoundRobinRouter(nrOfWorkers)), name = "workerRouter")

        def generate_random_intercept_point(): Double = {
          // Semi random generation based on number of x,y points
          return (
            numberList.size.toDouble /
            ((util.Random.nextInt(numberList.size) + 1) * (numberList.size + util.Random.nextInt(numberList.size))))
        }


        def receive = {
          case ComputeIntercept =>
            for(i <- 1 to nrOfWorkers + 1) {
              //get random initial intercept
              initialIntercept = Intercept(
                generate_random_intercept_point(), generate_random_intercept_point(), 0.0)

              if (i == 1) {
                // First execution loop computes initial value of cost function
                log.info("CalculateCost call for initial intercept theta0:{} theta1:{}",
                  initialIntercept.theta0, initialIntercept.theta1)
                workerRouter ! CalculateCost(initialIntercept, numberList)
              }

              workerRouter ! CalculateGradient(
                initialIntercept, numberList, learningRate, nrOfIterations)
            }

          case GradientResult(intercept) =>
            // calculate cost for every result
            log.info("CalculateCost call for intercept theta0:{} theta1:{}", intercept.theta0, intercept.theta1)
            workerRouter ! CalculateCost(intercept, numberList)

          case CostResult(result) =>
            log.info("CostResult ...")
            partialResults.append(result)
            nrOfResults += 1
            if (nrOfResults == nrOfWorkers + 1) {
              // Find the point with minimum error
              var error: Double = partialResults{0}.cost
              var position: Int = 0
              for (i <-0 to partialResults.size -1) {
                log.info("partial cost {} i {}", partialResults{i}.cost, i)
                if (partialResults{i}.cost < error) {
                  error = partialResults{i}.cost
                  position = i
                }
              }

              // Send the result to the listener
              listener ! InterceptResult(
                partialResults{position}, duration = (System.currentTimeMillis - start).millis)

              // Stops this actor and all its supervised children
              context.stop(self)
            }
        }
      }

      class Listener extends Actor {
        val log = Logging(context.system, this)

        override def preStart() = {
          log.info("Starting listener")
          super.preStart()
        }

        def receive = {
          case InterceptResult(intercept: Intercept, duration: Duration) =>
            log.info("total duration {}", duration)
            log.info(
              "GradientDescent final result theta0: {} theta1: {} cost: {}",
              intercept.theta0, intercept.theta1, intercept.cost)
            context.system.shutdown()
        }
      }


      def calculate(nrOfWorkers: Int, nrOfIterations: Int, learningRate: Double, fileName: String) {
        val numberList = Source.fromFile(fileName).getLines().map(
          s => s.split(",")).map(x => Point( x{0}.toDouble, x{1}.toDouble)).toList

        // Create an Akka system
        val system = ActorSystem("GradientDescentSystem")

        // create the result listener, which will log the result and will shutdown the system
        val listener = system.actorOf(Props[Listener], name = "listener")

       // create the master
        val master = system.actorOf(Props(new Master(
          nrOfWorkers, nrOfIterations, learningRate, numberList, listener)),
          name = "master")

        // start the calculation
        master ! ComputeIntercept

      }

       // Main program
      override def main(args: Array[String]) {
        val parser = new ArgotParser("gradient", preUsage=Some("Version 1.0"))
        val fileName = parser.option[String](List("f", "filename"), "filename", "filename")
        val nrOfWorkers = parser.option[Int](List("w", "workers"), "workers", "workers")
        val nrOfIterations = parser.option[Int](List("i", "iterations"), "itetarions", "iterations")
        val learningRate = parser.option[Double](List("r", "rate"), "rate", "rate")
        parser.parse(args)
        calculate(
          nrOfWorkers=nrOfWorkers.value.get, nrOfIterations=nrOfIterations.value.get,
          learningRate=learningRate.value.get, fileName=fileName.value.get)
      }
    }

</pre>

<h2> Ejecutando la aplicaci&oacute;n </h2>

<p>
   Voy a usar  <a href="https://github.com/maigfrga/blog/blob/master/2015/03/linear-regession/total_by_period.csv">el set de datos</a>  que utilic&eacute; en mi   <a href"http://www.maigfrga.ntweb.co/regresion-lineal-adopcion-internet-en-colombia/"> post previo sobre regresi&oacute;n linear. </a>. 
   En ese post utilic&eacute; <a href="https://gnu.org/software/octave/"> Octave </a>  para calcular la 
   funci&oacute;n de costo para mi set de datos, obtiendo los valores θ0 = 2.1368e+05 and θ1x =  2.3589e+03.
</p>


<p>
Para ejecutar la aplicaci&oacute;n akka como comando de consola se debe llamar el comando <code> sbt </code>.
</p>

<pre>
 sbt "run --rate 0.0002  --iterations 200000 --workers 4 --filename ../total_by_period.csv"
</pre>

<p>
    A continuaci&oacute;n los resultados de ejecutar el algorimto con diferentes par&aacute;metros:
</p>

<table class="table">
      <thead>
        <tr>
          <th>Taza de aprendizaje</th>
          <th>N&uacute;mero de iteraciones</th>
          <th>N&uacute;mero de Workers</th>
          <th>Resultado  θ0 </th>
          <th>Resultado θ1</th>
          <th> Costo</th>
          <th>Tiempo de ejecuci&oacute;n</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>0.0001 </td>
          <td> 1000 </td>
          <td> 2</td>
          <td> 13444.522450507255</td>
          <td>15733.153261356721</td>
          <td> 1.4915513928506067E10 </td>
          <td> 82 milliseconds </td>
        </tr>
        <tr>
          <td>0.0001 </td>
          <td> 10000 </td>
          <td> 2</td>
          <td>96714.66045646855</td>
          <td>10171.263617220522</td>
          <td>  6.991494692323004E9 </td>
          <td>170 milliseconds</td>
        </tr>
        <tr>
          <td>0.0001 </td>
          <td> 10000 </td>
          <td>4</td>
          <td> 96714.64937053746   </td>
          <td>10171.264357686756   </td>
          <td> 6.9914914322857065E9   </td>
          <td> 290  milliseconds </td>
        </tr>
        <tr>
          <td>0.0002 </td>
          <td> 10000 </td>
          <td>4</td>
          <td>149323.8010837764   </td>
          <td> 6657.324029165672 </td>
          <td> 4.129770277424713E9 </td>
          <td> 250 milliseconds </td>
        </tr>
        <tr>
          <td>0.0001 </td>
          <td> 100000 </td>
          <td>4</td>
          <td>213137.42909120218</td>
          <td>2394.999439961037</td>
          <td>2.8873926496464314E9</td>
          <td>795 milliseconds</td>
        </tr>
        <tr>
          <td>0.0002 </td>
          <td> 100000 </td>
          <td>4</td>
          <td>213677.0403639862</td>
          <td>2358.957007201501</td>
          <td> 2.887304849208148E9 </td>
          <td> 854 milliseconds  </td>
        </tr>
        <tr>
          <td>0.0002 </td>
          <td> 200000 </td>
          <td>4</td>
          <td>  213678.41665775442 </td>
          <td>2358.865079960366 </td>
          <td> 2.887304848639881E9   </td>
          <td>997 milliseconds  </td>
        </tr>
        <tr>
          <td>0.0002 </td>
          <td> 400000 </td>
          <td>4</td>
          <td>213678.41666654532</td>
          <td>2358.8650793731726</td>
          <td>2.887304848639881E9   </td>
           <td>861 milliseconds  </td>
        </tr>
      </tbody>
</table>

<p>
    La informaci&oacute;n obtenida sugiere que el algoritmo funciona mejor con 20 mil iteraciones,
    iteraciones adicionales no aparentar ser muy significativas, los resultados puede varias con 
    un set de datos diferente.
    <a href="https://github.com/maigfrga/blog/tree/master/2015/04/gradient-descendant-akka"> El 
    c&oacute;digo fuente de este ejemplo puede ser descargado en este enlace.</a>
</p>
<h2> Referencias </h2>

<ul>
    <li>
        <a href="https://www.coursera.org/course/ml"> Machine Learning Coursera. </a>
    </li>

    <li>
     <a href="http://en.wikipedia.org/wiki/Gradient_descent"> Gradient Descent Wikipedia. </a>
    </li>
    <li>
        <a href="https://www.youtube.com/watch?v=umAeJ7LMCfU"> Gradient Descent - Artificial Intelligence for Robotics.  </a>
    </li>
    <li>
        <a href="https://www.youtube.com/watch?v=ykeCKm9JqSA">
             Calculating the Gradient of a Straight Line.
        </a>
    </li>
    <li>
        <a href="http://betterexplained.com/articles/understanding-pythagorean-distance-and-the-gradient/">
            Understanding Pythagorean Distance and the Gradient.
        </a>
    </li>
    <li>
        <a href="http://betterexplained.com/articles/vector-calculus-understanding-the-gradient/"> Vector Calculus: Understanding the Gradient.   </a>
    </li>
    <li>
        <a href="http://betterexplained.com/articles/measure-any-distance-with-the-pythagorean-theoremx/">
            How To Measure Any Distance With The Pythagorean Theorem.
        </a>
    </li>
    <li>
         <a href="http://www.maigfrga.ntweb.co/akka-toolkit-introduction/"> Akka Toolkit Introduction. </a>
    </li>
    <li>
        <a href="http://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/"> An Introduction to Gradient Descent and Linear Regression. </a>
    </li>
</ul>
